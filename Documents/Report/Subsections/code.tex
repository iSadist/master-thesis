\newpage

\begin{appendices}
\chapter{Code from chapter Augmented Reality}
\section{}
\begin{lstlisting}[language=swift]
func loadWorldTrackingConfiguration()
    {
        let configuration = ARWorldTrackingConfiguration()
        configuration.planeDetection = [.horizontal]

        // All the objects that are tracked is contained in the Objects folder
        guard let detectingObjects = ARReferenceObject.referenceObjects(inGroupNamed: "Objects", bundle: nil) else { return }
        configuration.detectionObjects = detectingObjects
        
        // Setting up tracking of images
        for imageURL in trackingImageURLs
        {
            guard let image: CGImage = UIImage(named: imageURL)?.cgImage else { return }
            let referenceImage = ARReferenceImage(image, orientation: CGImagePropertyOrientation.up, physicalWidth: 0.3)
            configuration.detectionImages.insert(referenceImage)
        }

        configuration.maximumNumberOfTrackedImages = trackingImageURLs.count
        
        // Running the sessoin with the configuration
        sceneView.session.run(configuration)
    }
\end{lstlisting}

\section{}
\begin{lstlisting}[language=swift]
// Load the scene
let scene = SCNScene(named: "art.scnassets/world.scn")!        
sceneView.scene = scene
\end{lstlisting}

\section{}

\begin{lstlisting}[language=swift]
func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor)
    {
    	    // If detected an object
        if let objectAnchor = anchor as? ARObjectAnchor
        {
        	   // Add some 3D text to the scene
            let objectName = objectAnchor.referenceObject.name!
            let textNode = GeometryFactory.makeText(text: objectName)
            node.addChildNode(textNode)
        }
        // If detected a plane
        else if let planeAnchor = anchor as? ARPlaneAnchor
        {
        // Add a plane geometry of the detected floor to the scene
        node.addChildNode(GeometryFactory.createPlane(planeAnchor: planeAnchor, metalDevice: metalDevice!))
            model.numberOfPlanesDetected += 1
        }
    }
\end{lstlisting}

If nodes need to be rendered outside of this function it can be done by accessing the
scenes root node.

\begin{lstlisting}[language=swift]
sceneView.scene.rootNode.addChildNode(node)
\end{lstlisting}

\chapter{Code from chapter Neural Networks}
\section{}
\begin{lstlisting}[language=python]
#Python script from training
#Project path:
	master-thesis/Training/trainer.py
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.utils import shuffle

train_images = []
train_labels = []
loadImages(train_images, train_labels, "Train", 200)
train_images = reshapeArray(train_images)

test_images = []
test_labels = []
loadImages(test_images, test_labels, "Test", 39)
test_images = reshapeArray(test_images)

# Create the neural network
model = keras.Sequential([
	keras.layers.Conv2D(4, kernel_size=(5, 5), strides=(2, 2), input_shape=(image_height, image_width, number_of_color_channels)),
		
	["The code for the hidden layers"]

	keras.layers.Dense(4, activation=tf.nn.softmax)
])

model.compile(optimizer=keras.optimizers.Adam(),
	    loss='sparse_categorical_crossentropy',
	    metrics=['accuracy'])
	    
train_data, train_labels = shuffle(train_data, train_labels)

early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1)
checkpoint = keras.callbacks.ModelCheckpoint("./Models/Nolmyra.h5", monitor='val_acc', 
verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)

history = model.fit(train_data, train_labels, epochs=40, batch_size=10, validation_data=(test_images, test_labels), callbacks=[early_stopping, checkpoint] , verbose=1)

model.save("./Models/recognizer.h5")
\end{lstlisting}

\section{}
\begin{lstlisting}[language=python]
Conv2D(4, kernel_size=(5, 5), strides=(2, 2), input_shape=(image_height, image_width, number_of_color_channels)),
Conv2D(4, kernel_size=(3, 3), strides=(1, 1), input_shape=(image_height, image_width, number_of_color_channels)),
MaxPool2D(pool_size=(2, 2), padding="valid"),
BatchNormalization(),
LeakyReLU(),
Conv2D(8, kernel_size=(3, 3), strides=(1, 1)),
Conv2D(8, kernel_size=(3, 3), strides=(1, 1)),
Conv2D(8, kernel_size=(3, 3), strides=(1, 1)),
MaxPool2D(pool_size=(2, 2), padding="valid"),
BatchNormalization(),
LeakyReLU(),
Conv2D(16, kernel_size=(3, 3), strides=(1, 1)),
Conv2D(16, kernel_size=(3, 3), strides=(1, 1)),
Conv2D(16, kernel_size=(3, 3), strides=(1, 1)),
MaxPool2D(pool_size=(2, 2), padding="valid"),
BatchNormalization(),
LeakyReLU(),
Flatten(),
Dropout(0.5),
Dense(64, kernel_regularizer=keras.regularizers.l2(0.003), activation=tf.nn.relu),
GaussianNoise(0.2),
Dense(64, kernel_regularizer=keras.regularizers.l2(0.003), activation=tf.nn.relu),
Dropout(0.25),
Dense(4, activation=tf.nn.softmax)
\end{lstlisting}

\section{}
 \begin{lstlisting}[language=python]
#Project path: master-thesis/FeatureTraining/transferLearning.py
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input, Conv2D, MaxPool2D
from keras import backend as k
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping

#Load data and pretrained model
img_width, img_height = 256, 256
train_data_dir = "data/train"
validation_data_dir = "data/val"
nb_train_samples = 129
nb_validation_samples = 21
batch_size = 16
epochs = 50
input_layer = Input(shape=(256,256,3))
model = applications.VGG16(include_top=False, weights='imagenet', input_tensor=input_layer, pooling=None)

#Cut network and add own layers
x = model.get_layer('block5_pool').output
x = Flatten()(x)
x = Dense(512, activation="relu")(x)
predictions = Dense(4, activation="softmax")(x)

# creating the composed model
model_final = Model(inputs = model.input, outputs = predictions)
for layer in model_final.layers[:-2]:
    layer.trainable = False
    
# compile the model
model_final.compile(loss = "categorical_crossentropy", optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9), metrics=["accuracy"])

#Hidden lines of code
.........

#Train the model
hist = model_final.fit_generator(
train_generator,
epochs = epochs,
validation_data = validation_generator,
callbacks = [checkpoint, early])
\end{lstlisting}

\chapter{Code from chapter Object Detection}
\section{}

\begin{lstlisting}[language=swift]
let configuration = ARWorldTrackingConfiguration()
guard let detectingObjects = ARReferenceObject.referenceObjects(inGroupNamed: "Objects", bundle: nil) else { return }
configuration.detectionObjects = detectingObjects
\end{lstlisting}

\chapter{Code from  chapter One Stage Detector}
\section{}
\begin{lstlisting}[language=python]
import turicreate as tc
tc.config.set_num_gpus(0)

# Load the data
train_data = tc.SFrame("Train_Data.sframe")

#Random split train data to get specific training size
train_data, unused_data = train_data.random_split(0.3)

test_data = tc.SFrame("Test_Data.sframe")

# Create a model
model = tc.object_detector.create(train_data)

# Evaluate the model and save the results into a dictionary
metrics = model.evaluate(test_data,metric='mean_average_precision')
print(metrics)

# Save the model for later use in Turi Create
model.save('Nolmyra030.model')

# Export for use in Core ML
model.export_coreml('Nolmyra030.mlmodel', include_non_maximum_suppression=False)
\end{lstlisting}

\section{}
\begin{lstlisting}[language=python]
#Load test data
test_data = tc.SFrame("Test_Data.sframe")

#Load created model
model = tc.load_model('Nolmyra.model')

# Save predictions to an SArray and draw predicted bounding boxes
predictions = model.predict(test_data)
predictions_stacked = tc.object_detector.util.stack_annotations(predictions)
image_prediction = tc.object_detector.util.draw_bounding_boxes(test_data['image'], predictions)

#Look through the predicted bounding boxes
image_prediction.explore()
\end{lstlisting}

\section{}
\begin{lstlisting}[language=swift]
    private func filterOverlappingPredictions(unorderedPredictions: [Prediction], nmsThreshold: Float) -> [Prediction]
    {
        var predictions = [Prediction]()
        let orderedPredictions = unorderedPredictions.sorted { $0.confidence > $1.confidence }
        var keep = [Bool](repeating: true, count: orderedPredictions.count)
        for i in 0..<orderedPredictions.count {
            if keep[i] {
                predictions.append(orderedPredictions[i])
                let bbox1 = orderedPredictions[i].boundingBox
                for j in (i+1)..<orderedPredictions.count {
                    if keep[j] {
                        let bbox2 = orderedPredictions[j].boundingBox
                        if bbox1.IoU(other: bbox2) > nmsThreshold {
                            keep[j] = false
                        }
                    }
                }
            }
        }
        return predictions
    }
    
    extension CGRect
{
    func IoU(other: CGRect) -> Float
    {
        let intersection = self.intersection(other)
        let union = self.union(other)
        return Float((intersection.width * intersection.height) / (union.width * union.height))
    }
}
\end{lstlisting}


\chapter{Code from chapter Object Tracking}
\section{}

\begin{lstlisting}[language=swift]
// Project path:
// master-thesis/Application/ObjectDetectionInAR/Assembler/ObjectTracker.swift

func track()
    {
    // Init all variables
        cancelTracking = false
        var trackingObservations = [UUID: VNDetectedObjectObservation]()
        var trackedObjects = [UUID: ObjectRectangle]()
        let requestHandler = VNSequenceRequestHandler()
        let boundingFrame = delegate?.getBoundingFrame()
        
        // Add the objects to track to the created lists above
        for object in objectsToTrack
        {
            let observation = VNDetectedObjectObservation(boundingBox: object.getNormalizedRect(frame: viewFrame))
            trackingObservations[observation.uuid] = observation
            trackedObjects[observation.uuid] = object
        }
        
        // Loop over until a cancel tracking request is made
        while true
        {
            if cancelTracking { break }
            
            var rects = [ObjectRectangle]()
            var trackingRequests = [VNRequest]()
            
            guard let frame = delegate?.getFrame() else {
                usleep(useconds_t(millisecondsPerFrame * 1000))
                continue
            }
            
            for trackingObservation in trackingObservations
            {
                // Create the requests
                let request = VNTrackObjectRequest(detectedObjectObservation: trackingObservation.value)
                request.trackingLevel = .fast
                trackingRequests.append(request)
            }

		   // Perform the requests
            try? requestHandler.perform(trackingRequests, on: frame, orientation: CGImagePropertyOrientation.up)

            for processedRequest in trackingRequests
            {
                // Handle the results from the requests
                guard let observation = processedRequest.results?.first as? VNDetectedObjectObservation else { continue }
                
                if observation.confidence > confidenceThreshold
                {
                    guard let object = trackedObjects[observation.uuid] else { continue }
                   // Set new bounding box
                    object.setNewBoundingBox(newBoundingBox: observation.boundingBox, frame: boundingFrame)
                    trackedObjects[observation.uuid] = object
                    trackingObservations[observation.uuid] = observation
                    
                    rects.append(object)
                }
            }

            DispatchQueue.main.async {
                rects = rects.sorted { $0.name! < $1.name! }
                self.delegate?.trackedRects(rects: rects)
            }
            
            // The tracking will stop if no observation has a high confidence value
            if rects.isEmpty
            {
                DispatchQueue.main.async {
                    self.requestCancelTracking()
                    self.delegate?.trackingLost()
                }
            }

            usleep(useconds_t(millisecondsPerFrame * 1000))
        }
        
        DispatchQueue.main.async {
            self.delegate?.trackingDidStop()
        }
    }
\end{lstlisting}

\chapter{Code from  chapter The Finished Application}
\section{}

\begin{lstlisting}[language=swift]
    var furniturePartNodes = [SCNNode]()

        for object in model.foundObjects
        {
            guard object.name != nil else { return }
            guard object.position != nil else { return }
            
            let furnitureNode = addFurniture(part: object.name!, position: object.position!)
            furniturePartNodes.append(furnitureNode)
        }
        
        var previousNode: SCNNode? = nil
        var previousAnchorPoint: SCNNode? = nil
        
        var nodeActions = [(SCNNode, [SCNAction])]() // A list for storing animations to run on a node later
        
        for node in furniturePartNodes
        {
            var actions: [SCNAction] = []
            actions.append(SCNAction.rotate(by: -CGFloat.pi / 2, around: SCNVector3(0, 0, 1), duration: 1))

            let anchorPoint = node.childNode(withName: ANCHOR_POINT, recursively: true)
            
            if anchorPoint == nil
            {
                if previousAnchorPoint != nil
                {
                    actions.append(SCNAction.move(to: previousAnchorPoint!.worldPosition, duration: 2))
                }

                previousNode = node
            }
            else
            {
                previousNode?.runAction(SCNAction.move(to: anchorPoint!.worldPosition, duration: 2))
                if previousAnchorPoint != nil
                {
                    actions.append(SCNAction.move(to: previousAnchorPoint!.worldPosition, duration: 2))
                    actions.append(SCNAction.move(by: node.worldPosition.substract(other: anchorPoint!.worldPosition), duration: 2))
                }

                previousAnchorPoint = anchorPoint
            }
            
            // HACK: Adds an extra action with no content at the end to make completion handler wait until the last action is done
            actions.append(SCNAction.move(by: SCNVector3Zero, duration: 1))
            
            nodeActions.append((node, actions))
        }
}
\end{lstlisting}

\end{appendices}
